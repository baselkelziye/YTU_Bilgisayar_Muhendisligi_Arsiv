# 11. Week - 13 December 2022 Tuesday

# Ödev
Ödevde Spark SQL ile bulduğumuz sonucu Cassandra ile tekrar bulacağız.

# Spark Programming - Spark SQL

* Allows SQL extensions based on MLlib
* Shark is being migrated to Spark SQL
* Primary database RDDs and secondary tables

* Spark SQL --> Dataframe (similar to Pandas, R dataframes)
* You can get from columns as collection of vectors (each column entry should have the same datatype)
* Speed of DF >> RDD --> Because of the defined table scheme


You can use sql queries like below;
sqlContext.sql("select count(1) from ...)

* Transformations are lazy (not computed immediately)


## Creating a DataFrame

Spark SQL provides two methods for creating a DataFrame from an RDD
* toDF and --> infers the scheme (bazen int türleri string olarak alabiliyor.)
* createDataFrame --> requires you to specify the scheme (bizim girmemiz gerekiyor)


sqlContext.read.text("readme.txt")


TODO: Search this and get the code from slides for project assignment. "The createDataFrame method takes two arguments"... 

## Distinct

$ --> means that column name

```
max($"price")
min($"price")
```

## Filter
The filter method filters rows in the source DataFrame using a **SQL expression** provided to it as an **argument**

```
// These identical
val filteredDF = customerDF.filter("age > 25")
val filteredDF = sql("SELECT name, age, gender FROM customerDF WHERE Age > 25")
```

## RDD Operations

