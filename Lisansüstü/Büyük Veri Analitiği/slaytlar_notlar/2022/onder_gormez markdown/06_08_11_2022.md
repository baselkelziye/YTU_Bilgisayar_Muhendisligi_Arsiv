# 6. Week - 8 November 2022 Tuesday

# Homework 1 - MapReduce

## Question 1 - Word Count Program
* İlk örnekte word count example ını run etmeniz lazım.
* kaggle.com/ dan fake-news.zip i al and put in the hdfs use map reduce
* You can find code from zip or you can find from the internet. Or you can write your own
* Get the screen shot and count the word.
* En çok hangi kelimeler kullanılmış.
* Çalıştırdığınız kodun screen shot ını koyun.


Due/Delivery Date: 22 November 2022
Publicly avaliable data set



### Question 2 - Mean Temperature
* National Climatic Data Center (NCDC) Records parse etme kodunu yazacağız.
* Ortalama yıllık sıcaklık neymiş onu bulacağız.
* We have to find avarage temperature from the data collected
* Fahrenaid to Celcius conversion
* You need to parse out the temperature information 
* Ortamınızı çalıştırılabilir hale getirmemiz gerekiyor.
* Yıllara göre ortalamaları print edecek
* Çalıştırdığınız .jar ın screen shot ları ve ortalama sıcaklık değerleri olacak.

## MapReduce

Hafta 5'te kaldığımız yerden devam ediyoruz.

Map Reduce ile 2 farklı fonksiyon yazarak bu işlemi halletmiş oluyoruz.
* Divide the work multiple computers
* Combine the works done with reducers and get the single final result
* In every map reduce, results automatically shuffle and sort the results before the aggregate values by keys.


* Mapper can generate result with 1 or more key/value pair with the same key and different value. In this case we can define combiner function which combine the values and generate one result. It reduces the data sending to the reducer, and as a result network traffic reduced.

Mapper Result: C = 3 and C = 6
Combiner (In Mapper Computer): C = 9
Sent data to reducer is "C = 9"

## MapReduce Code
* Job **object** collect specs
  * Where is the JAR file to distribute?
  * Type of the output pair
  * Mapper and Reducer classes
  * Input and output file formats
  * Input file(s), output directory

* ReduceClass
Extends Reducer, declaring the input and output pair types for the reduce methods

* Derived from Writable
Text, IntWritable, LongWritable all implement Writable. You can also derive your class.


# "Hello World": Word Count

```Java
// Pseudo Code
Map(String docid, String text:
    for each word w in text:
        Emit(w, 1);

Reduce(String term, Iterator<Int> values):
    int sum = 0;
    for each v in values:
        sum += v;
    Emit(term, value);
```

```Java
public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable> {

    // Object created 1 times and used multiple times
    private final static IntWriteable one = new IntWriteable(1);
    private Text word = new Text();

    public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
        String Tokenizer itr = new StringTokenizer(value.toString());
        while(itr.hasMoreTokens()) {
            word.set(itr.nextToken());
            context.write(word, one);
        }
    }
}

public static class IntSumReducer extends Reducer<Text, IntWritable, Text, IntWritable>{
    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable<IntWritable> values, Context contect) throws IOException, InterruptedException {
        int sum = 0;
        for (IntWritable val : values){
            sum += val.get();
        }

        result.set(sum);
        context.write(key, result);
    }
}

main() {
    Job job = new Job(conf, "Word Count");
    job.setJarByClass(WordCount.class);
    // Must
    job.setMapperClass(TokenizerMapper.class);
    job.setCombinerClass(IntSumReducer.class);
    // Must
    job.SetReducerClass(IntSumReducer.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    FileInputFormat.addInputPth(job, new Path (otherArg[0]));
    FleOutputFormat.setOutputPath(job, new path(otherArgs[1]));
    System.exit(job.waaitForcompleton(true) ? 0 : 1)
    // ....
}
```

### An Optimization: The Combiner
```Java
// Local Count for word counting
// Pseudo code
def combiner(key, values):
    output(key, sum(values));
```

### Multiple Inputs
* You can define 2 different mapper class (Csv Mapper class, and txt mapper class)

### Partitioning
Why? Feature jobs will only focus on subsets of the data

Partitioning Schemes:
* Time: hour, day
* Geography: ...
* ....

### Binning
Binning like a partitioner without reduce.

### Three Gotchas
* Avoid object creation if possible
  * Reuse Writable objects, change the payload
* Execution framework reuses value object in reducer
* Passing parameters via class statics


### Getting Data to Mappers and Reducers
* Configuration parameters (directly in the job object for parameters)
* Side Data
  * DistributedCache

### Complex Data Types in Hadoop
How do you implement complex data types?
The easiest way:
  * Encoded it as Text, e.g. (a, b) ? "a:b"
  * Use regular expression to parse exract data
  * Works, but pretty hack-ish

The hard way:
  * Define a custom implementation of Writable (Comparable)
  * Must implement: readFields, write (CompareTo)
  * Computationally efficient, but slow for rapid prototyping
  * WritableComparator hook

The other way:
  * Use JSON Format

### Debugging Hadoop
* First, take a deep breath
* Start small,start locally
* Build incrementally

### Code Execution Environments
* Plain Java
* Local (on your machine) excetuion
* Pseudo-distributed (on same machine)
* Fully-distributed mode

### Hadoop Debugging Strategies
* Good ol' system.out.println
* Fail on success
* Programming is still programming
  * Use Hadoop as the "glue"


### Preserving State

Mapper Object = State + Configure + Map + Close
Reducer Object = State + Configure + Reduce + Close

### Good Coding Style: Theme
* Avoid object creation
* Avoid buffering

### Importance of Local Aggregation
Operation needs communication and syncronization. To reduce network communication;
* You can use combiners to combine the data and reduce the network communication
* You have to evenly distribute the data to be processed. Otherwise this increase the total completion time

### Shuffle

* Done by automacitally. You don't worry about it
