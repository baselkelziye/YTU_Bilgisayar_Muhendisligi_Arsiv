# 10. Week - 6 December 2022 Tuesday

# Introduction to Apache Spark
Yeni slaytın başlangıcı bu konu.

## A Major Step Backwards?

Map Reduce doesn't work with iterations. Just process batch data.
Streaming doesn't work with MapReduce.

Streaming --> Real Time needs different mechanism.
* In MapReduce, JVM handles the data, this is take some time (about 29 second) to execute program. So this is not a real time processing.

* Map Reduce just work brute force approach.
  * Missing features: Bulk loader, indexing, updates, transactions...
* MapReduce is incompatible with DMBS tools
* MapReduce is great for one-pass execution (start from begining to end)

# Move it onto a Cluster
* In cluster settings, using of Map-Reduce and Hadoop is slow
* Hadoop writes to disk and complex
  * Read data from disk, write result to disk. Commonly spend 90% of time doing I/O.
  * It would be great if we put the data (frequently used) in the ram (caching) is speed up the process. MapReduce doesn't. Spark it does.
* 


* You have HDFS as a underlying mechanism in Spark. But you have also memory block.
* Data in HDFS copied to memory block needed and cached. If it is required, read and process from the memory.

## What we need is...
* Resilient
* Checkpointing
* Fast, does not always store disk
* Replayable
* Embarassingly Parallel

* 90% of big data jobs uses Spark today. You need MapReduce in hadoop for 10% of the data processing.

## Why better than Hadoop?
* In-memory as opposed to disk
* Data can be cached in memory or disk for future use.
* Fast: Up to 100 times faster as it is using memory as oppose to disk
* Easier than Hadoop while being functional, runs a general DAG
* APIs in Java, Scala, Python, R

## Why better than hadoop?
Spark stores the path (processing steps) in graph. You can go back to the previous steps if needed in failure circuimtances. This reduces time to processing data in failure circumtances. You don't need to apply succesfull processes.


## Scala

* A type system that make sense.

## Resilient Distributed Datasets (RDDs)
* Immutable (not changeable) datasets
* If you manipulate the data, this transformed to other RDDs
* Fault tolerance (RDDs can always be recomputed from stable disk storage)

## Fault Tolerance

```Scala
file.map(lambda rec : (rec.type, 1))
    .reduceByKey(lambda x, y : x + y)
    .filter(lambda (type, count) : count > 10)
```


```Scala
val sc = new SparkContext()
val lines = sc.textFile("log.txt") // RDD[String]

// Transform using standard collection operations
val errors = lines.filter(_.startsWith("ERROR"))

val messages = errors.map(_.split('\t')(2)) // lazily evaluated

messages.saveAsTextFile("errors.txt") // kicks off a computation
```

## Spark Driver and Workers

* A spart program is two programs:
A **driver program** and a **workers program**
* Worker programs run on cluster nodes or in local threads
* DataFrames are distributed across workers

## Operations on RDDs
* Transformations (lazy)
* map


# Spark Core
Spark Core
* Spark SQL
* MLlib Algorithms
* Spark Streaming

## MLlib Algorithms

Machine Learning Library (MLlib)
```
points = context.sql("select latitude)
```

## Spark Streaming

Real Time and continous data processing. 

## MLlib + SQL

df = context.sql("select latitude, longtitude")


## Spark SQL

// Run SQL statements
val teenagers = context.sql("SELECT name FROM people WHERE age >= 13 AND age <= 19")


