# 1. Week - 2 October 2023 Monday

# Intro

Prof. Dr. Banu Diri anlatıyor.  
diri@yildiz.edu.tr  
drbanudiri@gmail.com  

* Maile en geç 1 gün içerisinde cevap veririm.
* Cevap vermezsem spam a falan düşmüştür. Tekrar atın
* Zamanım varsa detaylı yazabilirim
* Zamanım yoksa, sonuç odaklı kısa bir mail yazarım.

Hiç derste görmediğim birisinin seminer ve projede karşıma gelmesini istemiyorum.

Yoklama kağıdına en çok kullanılan mail adresini yazacağız.

![data-comp google grubu](https://groups.google.com/g/datacomp24) oluşturulacak.  

Çok fazla devama önem vermeyen biri olduğunu söyledi ama artık önem verecekmiş.

* Dersin en sonunda seminer yapılacak.  
  * Makale verecek. Bunlar dışında araştırılarak da makale bulunabilir.
  * Son 4 hafta kadar seminer sunumu olacak
  * Seminerler online olarak yapılacak

* Ders slaytları [Avesis Banu Diri duyuruları](https://avesis.yildiz.edu.tr/diri/documents) altında bulunabilir.
* Final sınavı yapmıyor. Proje yapılacak. Final zamanında projelerin kontrolü yapılacak.
    * Önceden projesini bitiren varsa, projeyi gösterebilir.

* 1 tane vizemiz olacak
  * Vize haftasında yapmıyor.
  * Konu bittiği zaman vizeyi yapıyor. 
  * Sonrasında direkt seminer sunumlara geçiliyor.


* 1 tane ödev veriliyor olacak.
    * Run Length Coding veya Run Length Encoding (RLE) algoritması, görüntü ve text lerde de kullanılabilir.
    * Winrar içerisinde de ara algoritma olarak kullanılıyor.
    * Farklı türde resmi okumanızı isteyeceğim.
    * RLE Algoritmaların farklı yöntemlerini isteyecek
    * Başı ve sonu olabilen bir ürün çıkarmak önemlidir.
    * Bazı library ler kullanılabilir. Karekök, bitmap header çözümü gibi.
    * Input u verip output u alabiliyor olmam önemli.

# İçerik

Kaynak:  
Data Compression, D. Salomon

* Vize %20
* Seminer %20
* Ödev %20
* Final Proje %40

# Seminer
* 10 veya 15 dakika ile sınırlı olacak
* Şu makeleyi aldım. Şunu kullanmışlar. Yapmaya çalıştıkları şu. Şunları daha sonraki aşamalara bırakmışlar.
* Amacımız kabaca, hangi konuda hangi şekilde veri sıkıştırma kullanılmış bunu anlatmak.
* Makalenin IEEE formatında raporunu vereceğiz.
* Seminerde seçtiğiniz bir konu üzerinde çalışıp, proje olarak verebilirsiniz. Tabiki veri sıkıştırma ile ilgiliyse

# Ödev
* Ödev için 2 hafta verilecek. Gayet iyi bir süre.
* Son dakikaya bırakılmazsa yetişir.

# Proje
Final yerine geçecek.
Final saatinde proje raporu online yıldıza yüklenecek.
Kod üzerinden anlatım yapılacak. Ama satır satır değil. Genel olarak anlatılacak. Koda hakim olmanız gerekiyor.

Çok güzel olursa tr dizin dergipark üzerinden yayınlanması için çalışma yapabilirsiniz.

Renk sayısını azaltmak istiyorsun. Sıkıştırma oranını arttırmak için.

# Veri Sıkıştırma Nedir?

Yerden ve zamandan kazanmak amacıyla veriyi bir formdan başka bir forma dönüştürme işlemidir.

* Fixed Size Code: Tüm sembollerin aynı bit uzunluğunda kodlanması
* Redundancy: Sıkıştırılmış veri içerisindeki fazlalık
* Variable Size Code: Sembollerin farklı bit uzunlukları ile kodlanması

# Redundancy (R)

Entropi (H): Elindeki veriyi sıkıştırmak istiyorsan ortalama şu kadar bite ihtiyacın var. Örn: 2.65 bit/char

Input Entropi: H
Output Entropi: S
R: |H - S|

Input un entropisi ile compressed output un entropisi arasındaki fark redundancy yi temsil eder.

H ile S arasındaki fark redundancy yi temsil ediyor.

# Veri Sıkıştırmada Kullanılan Kavramlar

Finalden yani projeden örnek;
* huffman algoritmasıyla sıkıştırdı.
* Sıkıştırdım. %90 performansla. %80 in üzerinde text te tek algoritmayla pek mümkün değil.
* Sıkıştırdığın dosyayı geri aç bakalım denilince açabiliyor olman lazım. Açma işlemini yapamıyorsan bir sıkıştırmadan bahsedemeyiz.

## Compressor / Encoder / Kodlayıcı

## Decompressor / Decoder / Kod Çözücü

## Codec
Aynı anda encoder ve decoder'ı birlikte tanımlar

# Unencoded / Raw Data / Original Data

## Encoded /Compressed
Sıkıştırılmış veriye verilen isimdir.

# Sıkıştırma Yöntemleri

## Non - Adaptive Compression
* Parametre ve işlemler belli tip veriler için geçerlidir (facsimile compression)
* En düşük süreli olan compression algoritmasıdır.
* Çok hızlı sıkıştırır. Sıkıştırma performansları düşüktür.

## Adaptive Compression
* İşlenmemiş her türlü veri üzerinde işlem yapar.
* Parametre ve işlemler değiştirilebilir.
* Daha yavaş çalışır fakat performansı yüksektir.

## Semi - Adaptive Compression
Adaptive e göre süresi daha az. Ama başarı performansı düşüktür.

## Lossy / Lossless Compression

* Lossy: Görüntü ve ses sıkıştırmada kullanılır.
* Lossless: Bir çek verisi kayıplı sıkıştırılamaz.

## Symmetrical Compression
Compressor ve Decompressor aynı algoritmayı ters yönde kullanırlar

## Asymetric Compression
Compressor ve Decompressor farklı algoritmayı ters yönde kullanırlar

# Soru / Cevap

**Soru:** Kodlayıcının karışık ve uzun sürdüğü buna karşılık kod çözücünün basit ve hızlı çalıştığı sıkıştırma algoritmaları
* **Cevap:** Arşiv uygulamaları

**Soru:** Kodlayıcının basit ve kısa sürdüğü buna karşılık kod çözücünün karışık ve yavaş çalıştığı uygulama alanı
* **Cevap:** Backup uygulamaları. Her gün hızlı backup alınmalı. Eğer bir gün gerekirse (ki çoğu kez gerekmiyor) yavaş bir şekilde geri açabilirsin.

## Stream Mode / Block Mode
* Stream Mode: Veri byte byte çekilir ve üzerinde işlem yapılır.
* Block Mode:  Veri blocklar halinde çekilir ve her block üzerinde farklı algoritma çalıştırılabilir.

## Bits per Chars

## Bits per Pixels

## Sıkıştırma Oranı
* **Compression Ratio:** size of the output stream / size of the input stream

CR: 0.6 ise elimde 100 birim veri varsa 60 birim veri elde ettim.  
Makalelerde genelde CR bilgisi verilir.

# Sıkıştırma Performansı (Yüzdesel)
100 * (1 - commpression_ratio (CR))

## Corpus
Eğer bir sıkıştırma makalesi yaptıysan bunları örnek veri setleri üzerinde de çalıştırarak sıkıştırmanın performansının yüksek olduğunu ispatlıyor olman lazım.

Calgary, Canterbury, UCI

Calgary:  
Canterbury: İçine kod python konulmuş vb bir veri seti  
UCI: Görüntülerle ilgili veri seti

Yukarıdaki veri setleri üzerinde kendi algoritmanı dene. Bunun performanslarını bizimle paylaş ki, diğer çalışmalarla kıyaslansın.

Göksel hocanın tezinde kullanılan yöntemi ile ilgili; hakemler en iyi performans vermesi için önceden seçilmiş bir veriyle çalıştırmadığınızı nerden bileyim? şeklinde cevap döndüler.  
Yukarıdaki veri setlerini kullanarak tekrar denettirdiler.  
mode up veri seti var onda deneyin  
news letter veri seti var onu deneyin  

## Root Mean Square Error (eRMS)
Orjinal ve sıkıştırılmış veri arasındaki hata oranını verir.

I(r, c) orijinal resim  
I'(r, c) decompress edilmiş resim

## Signal Noise Ratio (SNR)

eRMS e bağlı olarak logaritmik bir veri üzerinde gösterilmesini sağlar.
