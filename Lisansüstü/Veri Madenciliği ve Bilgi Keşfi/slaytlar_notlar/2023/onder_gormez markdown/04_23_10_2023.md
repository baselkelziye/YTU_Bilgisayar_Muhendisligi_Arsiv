# 4. Week - 23 October 2023 Monday

Aşağıdaki istatistiksel ve lineer cebir konularını principale component analysis (PCA) konusunu anlayabilmek için bilmemiz gerekiyor.

# İstatistiksel Konular

## Ortalama (Mean)

Ortalamanın Formülü:  
$$ {\displaystyle A={\frac {1}{n}}\sum _{i=1}^{n}a_{i}={\frac {a_{1}+a_{2}+\cdots +a_{n}}{n}}} $$

## Standart Sapma (Standard Deviation)

Standart Sampanın Formülü:  
$$ s = \sqrt{\frac{1}{N-1} \sum_{i=1}^N (x_i - \overline{x})^2}  $$


## Varyans (Variance)
Standart sampanın karesi varyansı verir.

$$ var(x) = s^2 = \frac{1}{N-1} \sum_{i=1}^N (x_i - \overline{x})^2  $$

## Kovaryans (Covariance)

* Değişken tek parametreli değilse, değişkenin 2 parametresi arasındaki ilişkiyi görebilmek için kovaryansı kullanıyoruz.
* Formülü varyansa çok benzerdir. Varyanstaki karesini alma yerine ikinci değişken koyularak ifade edilir.

$$ cov(x,y) = \frac{1}{N-1} \sum_{i=1}^N (x_i - \overline{x}) (y_i - \overline{y})  $$

Kovaryansın pozitif çıkması, değişkenlerin birbiri ile doğru orantılı olduğunu gösterir. Örneğin ders çalışma süresi artarsa, not ortalaması da artar.

# Lineer Cebir Konuları

Lineer cebirde büyük harfler matrisi ifade ederken, küçük harfler vektörü ifade eder.

## Eigen Vector (Öz Vektör)
* Bir matrisi eigen vectör ile çarptığımızda çıkan sonucu yine eigen vectör cinsinden ifade edebilirim.
* Eigen vector olmayan vektörlerde aynı cinsten ifade etmek mümkün değildir.

$$ A = \begin{pmatrix}2 & 3 \\ 2 & 1\end{pmatrix} $$

$$ \vec{x} = \begin{pmatrix}3\\2\end{pmatrix} $$

$$ A * \vec{x} = \begin{pmatrix}12\\8\end{pmatrix} = 4 * \begin{pmatrix}3\\2\end{pmatrix}$$

Yukarıdaki eşitlikler göz önünde bulundurulursa x vektörü, A matrisinin bir eigen vektörüdür.
* Bir eigen vektörü sabit bir sayı ile çarpıp elde ettiğim sonuç yine bir eigen vektördür.

## Eigen Value (Öz Değer)
Eigen Value: Bir matris ile çarpıldığında çıkan sonucu yine kendi cinsinden ifade edebilen sabit sayıdır.

### Determinant
Determinant: Bir matrisin öz değerini bulmak için kullanılır.

$$ A = \begin{pmatrix}a & c \\ b & d\end{pmatrix} $$

$$ det(A) = a * d - b * c $$

# Principle Component Analysis (PCA)

* Feature reduction altında, feature composition kısmındaki, unsuperwised bir yöntemdir.
* [A tutorial on Principal Components Analysis, Lindsay I Smith](https://ourarchive.otago.ac.nz/bitstream/handle/10523/7534/OUCS-2002-12.pdf?sequence=1&isAllowed=y)
* İlk olarak istatistiksel konuların üzerinden bi geçelim.

0 8 12 20 ve 8 9 11 12 gibi 2 diziyi düşünelim.
* Ortalamaları aynı olmasına rağmen farklı özelliklere sahiplerdir.
* Buradaki farkı ifade edebilmek için standart sapmayı kullanırız.

* Eksen taşıma işinde değişimin en fazla olduğu yöne doğru taşıma yapılır.
* Veri setinizde 10 tane sütun varsa, 10 tane ekseniniz olur. Bunlarda indirgeme yapılması daha zor.
  * Kovaryan matrisi 10x10 olur.
  * 10 tane öznitelik yerine 1 tanesi %90 oranında değişimi ifade edebiliyorsa, 1 tane öznitelik ile işimizi görebiliriz.
  * 1 tanesi yetmiyorsa 2 tane için %90 kontrolü şeklinde ilerleriz. Bu 10 tane öznitelik yerine 2 tane öznitelik ile hesaplamalarımızı yapabiliriz demektir.

TODO: Teslimsiz ödev; hazır bir tool kullanmadan, küçük  bir python kodu yazarak eksenler üzerinde görüp analiz yapabilirsiniz.
