# 5. Week - 30 October 2023 Monday

# Decision Tree

[9 Kasım 2020 online dersinde](http://arsivonline.yildiz.edu.tr/Recordings/2020-2021/G%C3%9CZ%20D%C3%96NEM%C4%B0/BLM5116%20-%201/BLM5116%20-%201_9-11-2020_13-00_9-11-2020_15-50_5fcd5836-c106-4060-be82-75e754c4b32c.MP4) 13. dakikadan itibaren anlatılmaya başlandı.
* Telekomünikasyon veri seti ödevinden bahsediyor. Sonra başlıyor.
* Tam olarak 19. dakikada başlıyor anlatmaya.

* Training set ve test set olarak ikiye ayrılıyor.
* Bazı çalışmalarda validation set diye bir set daha kullanılıyor. Bu set ile modelin doğruluğu test ediliyor.
* Test setine ait grand truth değerleri biliniyor. Bu değerler ile modelin tahmin ettiği değerler karşılaştırılıyor ve modelin performansı ölçülüyor.
* Akış diyagramı benzeri (flowchart-like) bir yapısı var.
* Root Node: En üstteki düğüm.
* Leaf Node:
* 1970 li yıllarda geliştirilmiş bir algoritma.
* Eğiticili öğrenme noktasında en çok kullanılan algoritmalardan biri. Yeni çıkan yöntemlerin performansı, bu algoritmanın performansına göre ölçülüyor.

## Bilgi Kazancı (Information Gain)
* Bilgi kazancı en yüksek olan öz nitelik karar ağacının en üstüne yerleştirilir.
* İlgilendiğim veri bilgisayar satın alan insanlar olsun.
* Class Positive: Bilgisayar satın alanlar
* Class Negative: Bilgisayar satın almayanlar
* Toplam bilgi kazancı sınıftaki pozitif ve negatif değerlere göre hesaplanır.
* Bilgi kazancı hesaplandıktan sonra özniteliklerden bilgi kazancı en yüksek çıkan seçilerek karar ağacının en üstüne yerleştirilir.
* Alt düğümler oluşturulur. Bu alt düğümler için de bilgi kazancı hesaplanır.
* Alt düğümler pure (saf) olana kadar bu işlem devam eder.

## Entropy (Karmaşıklık)

* Entropy: Bir veri setinin içerisindeki bilginin karmaşıklığını ifade eder.
* Entropy değeri 0 olursa, veri seti pure (saf) olarak kabul edilir.

$$ Entropy (S) =  -(p_+ * \log_2(p_+) + p_- * \log_2(p_-))$$

* p_+ : Pozitif değerlerin oranı
* p_- : Negatif değerlerin oranı
* p_+ + p_- = 1 olmalıdır.
* S: Entropy değeri

# Desicion Tree Devam

* Elle yaptığımız algoritmalarda üst düğümde kullanılan öznitelikler alt düğümlerde kullanılmaz. Böylelikle hesaplama kolaylığı elde edilmiş olur.

# Overfitting

* Decision Trees that are **more complex than necessary** could result in overfitting.

Gen. Error (Model) = Train Error(Model, Train. Data) + alfa * Complexity(Model)

Complexity = k * Ntrain
Ntrain: Number of nodes in training
