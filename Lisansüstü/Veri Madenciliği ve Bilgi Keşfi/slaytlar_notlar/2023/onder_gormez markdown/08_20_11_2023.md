# 8. Week - 20 November 2023 Monday
* Bu hafta dahil 4 hafta kadar ders işliyor olacağız.
* Diğer kalan haftalarda proje sunumları olacak.

# Classification: Alternative Techniques

Lecture Notes for Chapter 4 (chap4_knn.pptx)
* Bu slayt özet bilgiler içeriyor.

* Entropy: Saflığı ölçer. 0'a yaklaştıkça saflık artar. 1'e yaklaştıkça saflık azalır.


## Instance Based Learning (K-Nearest Neighbors)
* Bir base classifier (temel sınıflayıcı) dır.
* Etiketlenmiş veriler var. Yeni gelen etiketsiz verinin ne olabileceğine etiketli verilerden bakarak karar veriyoruz.
* Tek en yakın komşuya göre karar vermek mantıksızdır. Çünkü seçtiğimiz örnek gürültü olabilir.
* 2 en yakın komşuya göre karar vermek ise eşitliğe neden olabileceği için mantıksızdır.
* 3 en yakın komşuya göre karar vermek **majority vote** ile karar vermeyi kolaylaştırdığı için mantıklıdır.
* Eğitim örneklerinin her biri için distance hesaplanıp bunları sort etmek durumunda kalıyoruz. Bu ciddi bir maliyet anlamına geliyor. bunu azaltmak için; 
  * K-D Tree yöntemi kullanılabilir.

While it’s not as popular as it once was, it is still one of the first algorithms one learns in data science due to its simplicity and accuracy. However, as a dataset grows, KNN becomes increasingly inefficient, compromising overall model performance. It is commonly used for simple recommendation systems, pattern recognition, data mining, financial market predictions, intrusion detection, and more. 

# Model Estimation and Classifier Accuracy Measures
Dataset class label ı olan örnekler. Bunları modeli eğitmek için 3 parçaya ayırırız.
* Train: Modeli eğitmek için ayrılır.
* Validation: Modelin performansını ölçmek için kullanılır. 
* Test: Grand truth (bildiğim değerler) ile karşılaştırılır. Modelin performansı ölçülür.

Model Overfitting: Modeli aşırı öğrendiğinde gerçekleşir.
* Eğitim örneklerinin sayısını arttırılarak kaçınılabilir.


# Data Splitting Methods

## Resubtitiution Method
* Tüm veriyi eğitim için, hemde test için kullanır.
* Başarım çok yüksek çıkıyor gibi olur ama modelin gerçek performansı hakkında bilgi vermez.

## Hold-out Method
* Training ve test data set olarak 2 ye ayrılır.

## Leave-one-out Method
* N tane veri varsa, N-1 tane veri ile eğitim yapılır. Kalan 1 veri ile test yapılır. Bu işlem N defa tekrarlanır.
* Her seferinde karar ağacı tekrar tekrar oluşturulur. Maliyetlidir.
* Sample sayısı az ise bu kullanılabilir. Çünkü sample ın tüm veriyi temsil etmesi daha kolaydır.

## K-Fold Cross Validation
* N tane veri varsa, N/K tane veri test için ayrılıar. Kalan veri ile eğitim yapılır. Bu işlem K defa tekrarlanır. Böylelikle tüm veri için test yapılmış olur.
* En yaygın kullanılan yöntemdir.

## Bootstrap Method
* Verinin modeline uygun olarak sahte veri üretilerek eğitim yapılır.

# K-Means Clustering

* K-Means Clustering: Unsupervised Learning (Eğitimsiz Öğrenme) dir.
* Requires data, but no class labels
* Useful when don't know what you're looking for
* Used for customer segmentations
* Bir veri etiketleme işi için alan uzmanları ile çalışmak gerekir. Bu da maliyetli ve uzun süren bir iştir.
  * Bir patalog veri etiketlemek için mevcut işinden zaman ayıramadığı için gecikebilir.
  * Bu gibi durumlarda K-Means Clustering kullanılabilir.

* Detect patterns
  * Customer shopping patterns
  * Regions of images

* Self- Organized Feature Map (SOFM)

## Commmon Distance Measures
* Euclidean Distance
* Manhattan Distance: also called taxicab norm or 1-norm distance

Birkaç tane daha yöntem var ama detayına değinilmedi.
* L1: City block distance
* L2: Euclidean distance

Temel Özellikler;  
* d(i, j) >= 0. Distance is always positive
* d(i, i) = 0. Distance from a point to itself is 0
* d(i, j) = d(j, i). Distance is symmetric
* d(i, j) <= d(i, k) + d(k, j). Distance satisfies the triangle inequality

SMC: Simple Matching Coefficient  
2 bit verisini bit bit karşılaştırdığımızda 0, 0 ve 1, 1 eşleşmelerinin tüm bitlere oranıdır.

Jaccard Coefficient  
The formula for calculating the Jaccard coefficient (J) is as follows:

J(A,B)= ∣A∪B∣ / ∣A∩B∣

Here:

* A and B are sets.
* ∣A∩B∣ represents the size (cardinality) of the intersection of sets A and B.
* ∣A∪B∣ represents the size of the union of sets A and B.

The Jaccard coefficient produces a value between 0 and 1, where 0 indicates no similarity, and 1 indicates complete similarity. The higher the Jaccard coefficient, the more similar the sets are.

# K-Means Clustering
Yukarıda verilen bilgilerin tüm hepsi K-Means Clustering hazırlığıydı.
* 4-5 iterasyondan sonra değerler küme merkezlerine doğru yakınsıyor.
* Küme merkezleri sabitleninceye kadar iterasyon devam eder.
* İterasyonlu işlemlerde termination point oluşturmamız gerekiyor. Bu durma noktasını biz belirliyoruz.
* Küme merkezleri sabitlendikten sonra, yeni gelen veriler için hangi küme merkezine yakın olduğuna bakılır.

Önümüzdeki hafta hiyerarşik kümeleme konusunu işleyeceğiz.