# 3. Week - 16 October 2023 Monday

# Data Reduction

* 03. Hafta - Data Reduction.ppt slaytı üzerinden gidiyoruz.

Çok büyük verileri işlememiz zor olacaktır. Bunu azaltmak için;
* İlgilenmediğimiz satırlar silebiliriz (sample reduction)
* İlgilenmediğimiz sütunları silebiliriz (feature reduction)
* 2 özellik birleştirilerek tek özellik (reduction) incelenebilir.
  * Kilo ve boy dan vücut ağırlık indexine dönüşüm.

Data reduction ın faydaları;
* Computing time kısalır
* Predictive/Descriptive accuracy artar
* Representation of the data-mining mode basitleşir, daha anlaşılır olur.

**Önemli Not:** Veri indirgeme yapıldığında yukarıdakilerden birinde kazanç sağlandığı zaman diğerlerinden birinde kayıp yaşanacaktır.

* Computing time dan kazanç, accuracy yi olumsuz etkiler.
* Modelin basitleştirilmesi, accuracy yi olumsuz etkiler.

Aşağıdaki ağaçta yer alan elemanlardan biri ile data reduction yapılabilir.

Data Reduction
* Feature Reduction
  * Feature Selection
    * Supervised
      * Based on mean and variance
    * Unsupervised
      * Entropy measure for ranking features
  * Feature Composition
    * Unsupervised
      * PCA: Principle Component Analysis
* Sample Reduction
* Value Reduction
  * Feature Discritization
    * Chi Merge Technique

## Feature Reduction

### Feature Selection

#### Based on mean and variance
Önemli özelliklerin seçilmesi
Özelliğin sonuçlarının ortalaması alınarak önemli olup olmadığına karar verilebilir.
* A ve B sınıflarının dataları için çıkan ortalamalar birbirinden olabildiğince uzak olmalıdır ki, bu özelliğin A'yı mı B'yi mi temsil ettiğini anlayabilelim.
* Her biri için bu şekilde bir test ile özelliklerin hangisini çıkaracağımıza karar verebiliriz.

#### Entropy measure for ranking features

Normalize Öklid Mesafesi: Formül kağıtta.
Hamming Distance: 

* Tüm veri setinin entropisi ile özelliğin dışarı çıkarıldığında oluşan entropi değeri arasındaki farka bakarak özelliğin önemini anlayabiliriz.
* Tüm özniteliklerin farkına bakılır. Fark en az olan öznitelik veri seti için en önemsiz özniteliktir.
* Bu özniteliği çıkarmayı düşünebiliriz.


### Feature Composition
Eigen value, eigen vector, covariance matrix gibi konular olduğu için önümüzdeki haftalarda detaylı anlatılacak.

## Value Reduction
* Örneğin: Feature lar çok büyük bir aralıkta değer alıyorsa, bunları 0 ile 1 arasına indirgeyebilirim.
* Mesela yaşlar yerine (10, 20, 33 vb.), genç, orta yaşlı, yaşlı gibi kategorilere ayırabiliriz.
* Cut Points: Geçiş noktaları. Yaş örneğinde kaç yaş orta yaşlı kabul edilecek.

### Feature Discritization
* Chi Merge Technique: Contingouns Table denilen bir tablo oluşturulur.
  * Ki-kare testi ile bu tablo üzerinden birleştirme işlemi yapılır.

  * TODO: Formülü internetten bul ve buraya ekle

```latex
X^2 = Σ [ (O-E)^2 / E ]
```
Burada:

X^2: Chi-Square değeri
Σ: Toplama işlemi
O: Gözlenen (observed) frekans
E: Beklenen (expected) frekans
Bu formül, her bir kategori için (O-E)^2 / E değerlerinin toplamını hesaplar. Bu değer, Chi-Square dağılımına göre anlamlılık testi yapmak için kullanılır.

  * İteratif bir yöntemdir. Her aralığı bir sonraki aralıkla birleştirilebilir mi testini yapıyoruz.
  * TODO: "chi-merge algorithm" yazınca co-pilot kodunu verir.
