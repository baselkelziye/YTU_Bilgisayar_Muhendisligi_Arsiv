# 5. Week - 3 November 2022 Thursday

## Scaling
* Resimlerde köşe olarak bilinen yerler ölçekleme yapılırsa kenar olabilir. Bu durumda 2 resmin aynı olduğunu tespit edebilmek için aldığımız çerçevenin boyutunu büyütmemiz gerekir.
* Otomatik olarak doğru ölçeği seçebilmemiz gerekir.

## Automatic Scale Detection

* Difference-of-Gaussian (DoG) uzayında 2 resmin arasındaki ölçek farkını arayabiliriz.

Aşağıdaki şekilde farklı sigma değerleri için gaussianlarını elde etmiş oluyoruz.
orjinal sigma
sigma^2
sigma^3
sigma^4
sigma^5

* Küçük bir resmi büyütmek kolay değil. Detaylar random bir şekilde geleibilir. Bunun yerine hep büyük resimleri küçültüyoruz. Böylelikle ilk gaussian ımız hep orjinal resmimiz oluyor.



# Bölge ve Nokta Bulucu Comparision

Harris, Hessian, MSER, LoG (Laplacian of Gaussian olabilir)

## Image Representation: Histograms



### Lowe
* Oluşturduğu matrise SIFT tarif edicisi diyor.
* Gaussian la biraz yumuşatıyor. Böylelikle merkeze yakın değerler daha belirginleşiyor.
* Histogramlar titreşimlere de nispeten dayanıklılar.

TODO: SIFT haricindeki diğer algoritmalara (SURF, Shape Context, etc) detaylı bakmanıza gerek yok. Vize veya finalde soru sorarsam SIFT ten sorarım.


### Local Descriptors: SURF


### Local Descriptors: Shape Context


# Feature Matching and Robust Fitting
NOTE: Bu yeni slaytın ismi

Aşağıdaki algoritmaları inceleyeceğiz.
* Least Squares Fit
* Robust Least Squares

## Least Squares Line Fitting
$$E = \sum_{i = 1}^n (y_i - mx_i -b)^2$$

See for python example: numpy.linalg.lstsq

TODO: Slayt 26 da bu denklemin kendisi var. Bunun üzerinden geçen yıllarada sorular sorulmuş. Bu yılda vize veya finalde sorduğunda yapabiliyor olmamızı bekliyor.

TODO: Total Least Squares ten vize veya finalde sorumlu değiliz. Slayt 27-31. Zaten taralı sanki gizlenmiş gibi slaytlar. Ordan anlarsın.

TODO: Least Squares in sorunlarını çözmek için geliştirilmiş bir yöntem olan Robust Least Squares yöntemini araştır.

* Elinizde non-linear bir denklem varsa, analitik çözümler bulamayabiliyoruz.

$$y = ax^2 + blogx + c$$
Bu hata değerlerini minimize eden a, b ve c değerlerini bulmamız gerekiyor. Bunun için;
* Partical Solution
* Differantial Equation
gibi yöntemler kullanılabiliyor.
