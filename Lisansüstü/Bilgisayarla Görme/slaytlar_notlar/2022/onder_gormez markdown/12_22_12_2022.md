# 12. Week - 22 December 2022 Thursday

## Training Neural Networks
Justin Johnson Lecture 10 sunumlarından devam ediyoruz.

axon from a neuron  
synapse  
dentrite  
output axon  

Activation function olarak biz ReLU yu gördük. Ama diğer aktivasyon fonksiyonları da var.

# Sigmoid

Saturated neurons "kill" the gradients  
Sigmoid yerine artık ReLU kullanılıyor.  

# Activation function: ReLU
* Very computationally efficient
* Converges much faster than sigmoid/tanh in practice (e.g. 6x)

## Leaky ReLU
Leaky ReLU daha iyi küçük bir bias ile başladığı için ölmüyor.
* Will not "die"

## Exponential Linear Unit (ELU)
* All benefits of RELU
* Adds some robustness to noise

## Scaled ELU
Scaled version of ELU that works better in deep networks

## Gaussian Error Linear Unit (GELU)
* Sıfıra yakın yerlerde yumuşak geçiş sağlar.
* Very common in Transformers (BERT, GPT, ViT)

BERT: Natural Language Processing için kullanılıyor.

## Activation Functions: Summary
* Don't think too hard. Just use **ReLU**
* All of them have similar results.

## Data Processing
* Before normalization; classification loss very sensitive
* After normalization; Less sensitive to small changes

## Weight Initialization
* Hepsini sıfırla başlatalım? İyi bir fikir değil.
* Küçük rastgele sayılar atayalım? Küçük network ler için çalışıyor. Büyük networkler için sorun oluyor.


# Regularization: Dropout
* Belirlenen kriterleri sağlamayanları atıyoruz.
* Dropout fully connected layer olan network lerde kullanılıyor.

## Data Augmentation
Veri arttırımı

* Resmin sağa, sola döndürülmesi
* Rasgele bir kısmının alınması
* Magnitude change, contrast change

gibi yöntemlerle eğitim setinin genişletilmesi neural network ün daha doğru sonuçlar vermesini kolaylaştırır.

## Regularization
* Fractional Pooling: 
* Cutout: Resmin bir kısmını keserek yerine anlamsız bir şeyler ile doldurma
* Mixup: Kedi köpek resimlerini karıştırılıyor.
* CutMix: Kedi imgesinin  rastgele kesilen yerlerine köpek resmi ekleme
* Label Smooting:


* Use DropOut for large fully-connected layers
* Data augmentation always is a good idea
