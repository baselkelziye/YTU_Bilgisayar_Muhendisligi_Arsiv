# 11. Week - 15 December 2022 Thursday

Lecture 7 - 11 Slaytları üzerinden devam ediyoruz.  
Geçen haftadakine benzer konuları farklı bir slaytta işliyoruz. Bir nevi tekrar gibi.

## Fully Connected Layer

32 x 32 x 3 image => strect to 3072 x 1 vector oluşuyor.

## Convolutional Layer

32 x 32 x 3 image  
aşağıdaki filtreye sokuluyor  
dot product a giriyor.  
3 x 5 x 5 filter  

b: bias  
WTx + b

Filtreyi kaydıra kaydıra oluşturduğumuza activation map diyoruz. Yukarıdaki işlemlerden 1 x 28 x 28 boyutunda activation map oluyor.  
Bir filtre kullanmıyoruz. 6 tane filtre kullanırsak 6 tane activation map oluşuyor. 6 x 28 x 28 boyutunda bir çıkış imajı oluşmuş oluyor.  
Eğitim sırasında kernellerin ve bias ın değerlerini activation map ten öğreniyoruz.

Batch of images ---> Batch of outputs

N imge sayısı  
Cin giriş kanal sayısı  
H yükseklik  
W genişlik  

N x Cin x H x W şeklinde bactch image temsil ediliyor.

## Stacking Convolutions

Arka arkaya convolution alınıdığı zaman her konvolüsyonda oluşan hidden layer larda boyut azalır.

ReLU ile non-linear lik oluşturuluyor.  

Input: 7x7  
Filter: 3x3  
Output: ?  

Output: W - K + 1  
W: width  
K: kernel filter  

7 - 3 + 1 --> 5x5 lik bir çıkış

Önemli Not: Çok fazla konvolutiona sokacaksak imajın küçülerek yok olmasını engellemek için zero padding yapılması gerekiyor.

Output: W - K + 1 + 2P  
P: padding  

7 - 3 + 1 + 2 = 7 x 7 lik bir çıkış

## Receptive Fields
Gördüğü alan anlamına gelmektedir.

## Strided Convolution
Atlama sayısını belirleyebiliyoruz. Böylelikle 2 2 atlayarak daha küçük bir çıkış elde ediyoruz.

S: Strideds  
Input: 7x7  
Filter 3x3  

Output: (W - K + 2P) / S + 1
(7 - 3 + 2 * 2) / 2 + 1 = 5 x 5 lük bir output 

TODO:  Yukarıdaki hesapları yapabilmek önemli

## Convolutional Example
Kaç parametre öğrenmemiz gerekiyor sorusu:  
Input Volume: 3 x 32 x 32  
10 tane 5 x 5 filters with stride 1, pad 2  

Output Volume Size: (W - K + 2P) / S + 1 ==> (32 - 5 + 2 * 2) / 1 + 1 ==> 32 olur yani 10 x 32 x 32 output oluşur.  
Filtreleri ve inputun kanallarını çarpıyoruz. ve filtre sayısını ekleyerek öğrenilecek parametre sayısını buluyoruz.  
Number of learnable parameters 5 x 5 x 10 = 250 bunu 3 ile çarpıyoruz 750 + 10 ekliyoruz. 760


Channel Sayısı x Filter + 1 (for bias)  
Parameters per Filter: 3 x 5 x 5 + 1 = 76  
10 filtre olduğu için bunları 10 ile çarpıyoruz.  
76 * 10 = 760  

flop: floating point operations
Modern işlemciler tek cycle da bir toplama ve bir çıkarma yapabiliyor. Bu nedenle kaç tane toplama ve çıkarma yapıldığını bularak ne kadar kaynak kullanacağını hesaplayabiliyoruz.

Output: 10 x 32 x 32 = 10240  
3x5x5 tensors (75 elements); total 75 * 10240 = 768K  
Her bir nokta kernel sayısıyla çarpılarak multiply-add operations hesaplanıyor.  

1x1 convolution: kanal sayısını arttırmak için kullanılan bir yöntemdir.

## Convolutional Summary

* Genel olarak Kh = Kw (Small square filters)
* Padding imge sayısını sabit tutacak şekilde ayarlanıyor.
* Kerneller 3, 5, 1, olabiliyor.

## Max Pooling

Kernel içerisinde bulunan tüm sayılardan maximum olanın değerini alıyoruz.

## Convolutional Networks

Convolutions --> Subsampling --> Fully connected --> Output

## Batch Normalization
Biz Local Contrast Normalization ı öğrendik.

* Makes deep network much easier to train.
* Batch normalization for fully-connected networks

## Layer Normalization

* Layer Normalition for fully connected networks

# CNN Architectures
Lecture 8 slaytlarından devam ediyoruz.

## ImageNet Challanges
Yarışmaya ilerleyen zamanlarda yenileri katılıyor.

### AlexNet
Önemli bir gelişme 2012 de deep learning in patlamasına neden oluyor.
* Error rate reduction (%16)
* 227 x 227 inputs
* 5 convolutional layer
* Max pooling
* 3 fully-connected layers
* ReLU nonlinearities
* 3 GB GPU

### VGG
* 2014
* Errror (%7.*)
* all conv 3x3 stride 1 pad 1
* parametre sayısı aşağıya iniyor.

### GoogleNet
* 2014
* Error (%6.*)

### ResNet
* 2015
* İnsan hatası zaten %5 civarında oluyor. Gözle bakıldığında
* Error %3.6
* 152 layer
* ResNet: Residual Network
* global avarage pooling 

A residual network is a stack of many residual blocks

### SeResNet
* Error %2.*
* 2017  
* Squeeze-and-Excitation Network  

this competiton moved;  
held after 2017 -> now moved to Kaggle

## Which Architecture should I use?

* Don't be a hero. For most problems you should use an off-the-shelf arachitecture; don't try to design your own!
* If you care about accuracy, **ResNet-50** or **ResNet-101** are great choices
* If you want an efficient network (real-time, run on mobile, etc) try **MobileNets** and **ShuffleNets**

## CS231n: Deep Learning for Computer Vision
TODO: Computer Vision ile uğraşacaksanız bu video eğitim serilerini izleyerek daha detaylı öğrenme sağlayabilirsiniz.


[Stanford University - CS231n: Deep Learning for Computer Vision](http://cs231n.stanford.edu/)  
Youtube da yayınlanmış dersler var;
* [Michigan Online - Justin Johnson](https://www.youtube.com/watch?v=dJYGatp4SvA&list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r)
* [Stanford University School of Engineering - Justin Johnson](https://www.youtube.com/watch?v=vT1JzLTH4G4&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)
