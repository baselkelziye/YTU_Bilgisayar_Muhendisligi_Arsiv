# 10. Week - 8 December 2022 Thursday

20.pptx slaytı üzerinden işliyoruz.

# Convolutional Neural Networks

## Fully Connected Layer

* Parametre sayısı çok artıyor.
* 2 Milyar a kadar çıkabiliyor.

## Locally Connected Layer

* Parametre sayısını azaltıyor.

## Convolutional Layer
* 1998 de ortaya atılmış bir fikir.
* Bir filtre oluşturuyoruz.
* Oluşturduğumuz filtreyi kullanarak fotoğrafı dolaşıyoruz.
* Daha önce işlediğimiz konvolüsyonla bir farkı yok.
* Parametre sayısı artık resimden bağımsız hale geliyor.
* 10K parameter

100 x 100 = 10K parametre oluşuyor.

h n-> j : Output feature map  
h n-1 -> k: input feature map  
w kj -> n: Kernel (Relu)  

Girişimizde kaç tane kanal varsa çıkış parametre sayımız buna bağlı oluyor.

## Key Ideas (Ana Fikirler)
* Standart nöral network kullanılırsa, imaj boyutuna göre parametre sayısı çok fazla artabiliyor.  

Solution:
* Connect each hidden unit to a small patch of the input
* Share the weight across space
* This is called convolutional layer.

## Pooling Layer
Convolutional network lerde pooling layer lar kullanılıyor.
Zaman içerisinde train edildikçe, filtreler göz bulucu haline gelmiş gibi durumlar oluşabiliyor.
* Pooling (e.g. taking max) filter responses at different locations we gain robustness.


## Max Pooling
n: layer  
j: channel  
9 kernel boyutundan max olanı alarak küçültüyorsak max pooling  
Max pooling en çok kullanılan yöntem.  
Filtreye en çok cevabı veren (filtreye benzeyen) çıktılar bir sonraki katmana geçebiliyor.

* Avarage Pooling, L2 Pooling, L2 Pooling Over Features gibi farklı yöntemlerde mevcut

## Pooling Layer: Receptive Field Size
Filtrelerde stride diye bir şeyde tanımlanabiliyor (shift amount). Kaydırma uzunluğu gibi düşünülebilir. Genelde kernel kaydırılırken 1 kaydırıyoruz. Ama 2 kaydıradabiliriz. O zaman stride 2 olmuş oluyor.

# Local Contrast Normalization
* Parlaklıktan kurtulmak için kullanılır.
* Kanallar ve layerlar üzerinden de hesaplanabilir.


# ConvNets: Typical Stage

Aşamaların sırası aşağıdaki gibi;  
Convolüsyon Al --> Rectification düzeltme uygula + Local Constrast Normalizasyonu Yap --> Pooling Yap

# ConvNets: Typical Architecture

ConvNets bizim için otomatik olarak kullanılması gereken algoritmaları buluyor.

# ConvNets: Training
Eğitirken ConvNets ler nöral networklerden bir farkı yok.  
Back Propagation: Türevi çıkararak geriye besliyoruz. Türevlerden yararlanarak optimizasyonlarımızı yapıyoruz.

İmageNet (slaytta 2012) çok büyük bir veri seti. Bu günümüzde hala kullanılıyor.

## Architecture for Classification
Linear ve Fully Connected farkı ne?
Linear katmanda vermiyoruz.

Her şey her şey ile bağlı.

Total 60M nöral parametre var. 37M ve 16M fully connected layer dan geliyor.

## Transfer Learning
* Çok güçlü makinesi olmayan kişiler daha önceden farklı kişilerin eğittiği modelleri kullanarak işlerini görmeye çalışıyorlar.
* Katman katman her parametreyi yayınlıyorlar.
* Kaç katman olmalı? Kaç parametreyi değiştireceğiz? Gibi konuları çözmek gerekiyor.

* https://huggingface.co/
* colab: Bağımsız olarak yayınlanıyor. Eğitilmiş model de var.
* https://pytorch.org/
* kaggle: İnsanlar verisetleri üzerinde yarışmalar düzenliyorlar

* Model kullanabileceğiniz bir şey ise direkt alıp kullanabiliyorsunuz. Direkt kullanamıyorsanız transfer learning yapıyorsunuz.
* Genelde son katmanları daha çok değiştiriyorsunuz. Çünkü çıktıya en yakın katman üst katmanlar oluyor. Kendi işimize uygun olarak üst katmanları kendi ihtiyaçlarımıza göre düzenliyoruz.
* Altta kalan özellikler zaten çizgi, nokta gibi ayrımları yaptığı için ortak oluyor.

## API Usage

* Transfer Learning haricinde hiç yeniden öğretmeye vs bulaşmadan kolay bir şekilde yazılımı kullanabiliyoruz.
* Son kullanıcıya göre bir program.

## Optimization

SGD with momentum:

Yıllar içerisinde eğitme işindeki optimizasyonlar 

* group out diye bir uygulama var. Birbiri ile ilişkili olanları bir çalıştırmada bağlarken, diğerinde bağlamayarak eğitimi daha başarılı hale getirme yöntemi.
* input distortion ile aynı input sağa sola rotasyon yaparak input sayısını farklı bir şekilde vererek eğitimi daha başarılı hale getirebiliriz.
* Weight Decay: L2 regularization minimize a loss function compromising both the primary loss function and a penalty on the L2 norm.

20.pptx slaytı bitti.

# Introduction to PyTorch

Introduction_to_PyTorch.pdf dokümanı üzerinden devam ediyoruz.   
Biraz hands on experience

## Why PyTorch
* GPU support
* Similar to numpy interface
* Türev almak çok kolaylaştırılmış.

* 3 different library in popular use (Numpy, Tensorflow, PyTorch)


## Example Code

```Python
import torch

N, D = 3, 4

# Derin Öğrenmede matrislere tensor deniliyor.
x = torch.rand((N, D), requires_grad=True)
y = torch.rand((N, D), requires_grad=True)
z = torch.rand((N, D), requires_grad=True)

a = x * y
b = a * z
c = torch.sum(b)

print(b)

# x, y ve z parametrelerini minimize ederek c yi optimize etmeye çalışıyoruz.
c.backward()

# grandyantı
x.grad

# c yi daha iyi elde etmek için a yı küçülttük.
x = x - 0.0001 * x.grad

c2 = torch.sum(x * y * z)
print(c2)
```