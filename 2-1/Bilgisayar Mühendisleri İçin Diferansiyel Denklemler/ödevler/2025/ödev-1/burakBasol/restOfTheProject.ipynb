{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13749ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time, os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9954f8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EMBEDDING_PICKLES = [\n",
    "\t\"soru_cevap_embeddings.pkl\",\n",
    "\t\"e5_large_embeddings.pkl\"\n",
    "]\n",
    "\n",
    "IMGS_DIR = \"imgs\"\n",
    "\n",
    "NUM_SEEDS = 5\n",
    "EPOCHS_MAIN = 200\n",
    "EPOCHS_MLP_SMALL = 30\n",
    "EPOCHS_MLP_COMPARE = 50\n",
    "LEARNING_RATE = 0.01\n",
    "SIZES = [0.05, 0.1, 0.2, 0.5, 0.8, 1.0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9410512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class actualModel:\n",
    "\tdef __init__(self, input_dim, initial_w=None) -> None:\n",
    "\t\tself.name = \"Normal\"\n",
    "\t\tif initial_w is None:\n",
    "\t\t\tself.w = np.random.randn(input_dim, 1) * 0.01\n",
    "\t\telse:\n",
    "\t\t\tself.w = initial_w.copy()\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\tlinear = np.dot(X, self.w)\n",
    "\t\treturn np.tanh(linear)\n",
    "\n",
    "\tdef loss(self, y_true, y_pred):\n",
    "\t\treturn np.mean((y_true - y_pred)**2)\n",
    "\n",
    "\tdef accuracy(self, y_true, y_pred):\n",
    "\t\tpredictions = np.sign(y_pred)\n",
    "\t\treturn np.mean(predictions == y_true)\n",
    "\n",
    "\tdef get_gradients(self, X_batch, y_batch, y_pred):\n",
    "\t\tN = X_batch.shape[0]\n",
    "\t\tdiff = (y_pred - y_batch)\n",
    "\t\tdtanh = (1 - y_pred**2)\n",
    "\t\tgrad = np.dot(X_batch.T, diff * dtanh) * 2 / N\n",
    "\t\treturn grad\n",
    "\n",
    "class TwoLayerMLP:\n",
    "\tdef __init__(self, input_dim, hidden_dim=64):\n",
    "\t\tself.name = f\"MLP-{hidden_dim}\"\n",
    "\t\tself.params = {\n",
    "\t\t\t'W1': np.random.randn(input_dim, hidden_dim) * 0.05,\n",
    "\t\t\t'W2': np.random.randn(hidden_dim, 1) * 0.05\n",
    "\t\t}\n",
    "\t\tself.cache = {}\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\tz1 = np.dot(X, self.params['W1'])\n",
    "\t\ta1 = np.tanh(z1)\n",
    "\t\tz2 = np.dot(a1, self.params['W2'])\n",
    "\t\ta2 = np.tanh(z2)\n",
    "\t\tself.cache = {'X': X, 'a1': a1, 'a2': a2}\n",
    "\t\treturn a2\n",
    "\n",
    "\tdef get_gradients(self, X, y_true, y_pred):\n",
    "\t\tN = X.shape[0]\n",
    "\t\ta1 = self.cache['a1']\n",
    "\t\tdelta2 = 2 * (y_pred - y_true) * (1 - y_pred**2) / N\n",
    "\t\tgrad_W2 = np.dot(a1.T, delta2)\n",
    "\t\tdelta1 = np.dot(delta2, self.params['W2'].T) * (1 - a1**2)\n",
    "\t\tgrad_W1 = np.dot(X.T, delta1)\n",
    "\t\treturn {'W1': grad_W1, 'W2': grad_W2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c0d8798",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Optimizer:\n",
    "\tdef update(self, params, grads): raise NotImplementedError\n",
    "\n",
    "class SGDOpt(Optimizer):\n",
    "\tdef __init__(self, lr=0.01):\n",
    "\t\tself.lr = lr\n",
    "\tdef update(self, params, grads):\n",
    "\t\tfor key in params:\n",
    "\t\t\tparams[key] -= self.lr * grads[key]\n",
    "\n",
    "class AdaGradOpt(Optimizer):\n",
    "    def __init__(self, lr=0.01, epsilon=1e-8):\n",
    "        self.lr, self.eps = lr, epsilon\n",
    "        self.G = {}\n",
    "    def update(self, params, grads):\n",
    "        for key in params:\n",
    "            if key not in self.G:\n",
    "                self.G[key] = np.zeros_like(params[key])\n",
    "            self.G[key] += grads[key]**2\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.G[key]) + self.eps)\n",
    "\n",
    "class RMSPropOpt(Optimizer):\n",
    "    def __init__(self, lr=0.001, decay_rate=0.9, epsilon=1e-8):\n",
    "        self.lr, self.dr, self.eps = lr, decay_rate, epsilon\n",
    "        self.E_g2 = {}\n",
    "    def update(self, params, grads):\n",
    "        for key in params:\n",
    "            if key not in self.E_g2:\n",
    "                self.E_g2[key] = np.zeros_like(params[key])\n",
    "            self.E_g2[key] = self.dr * self.E_g2[key] + (1 - self.dr) * (grads[key]**2)\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.E_g2[key]) + self.eps)\n",
    "\n",
    "class AdamOpt(Optimizer):\n",
    "\tdef __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "\t\tself.lr, self.beta1, self.beta2, self.eps = lr, beta1, beta2, epsilon\n",
    "\t\tself.m, self.v, self.t = {}, {}, 0\n",
    "\tdef update(self, params, grads):\n",
    "\t\tself.t += 1\n",
    "\t\tfor key in params:\n",
    "\t\t\tif key not in self.m:\n",
    "\t\t\t\tself.m[key] = np.zeros_like(params[key])\n",
    "\t\t\t\tself.v[key] = np.zeros_like(params[key])\n",
    "\t\t\tself.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[key]\n",
    "\t\t\tself.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (grads[key]**2)\n",
    "\t\t\tm_hat = self.m[key] / (1 - self.beta1**self.t)\n",
    "\t\t\tv_hat = self.v[key] / (1 - self.beta2**self.t)\n",
    "\t\t\tparams[key] -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebe8b247",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_generic(model, optimizer, X_tr, y_tr, X_te, y_te, epochs=30, batch_size=32):\n",
    "\thistory = {'loss': [], 'acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\tN = X_tr.shape[0]\n",
    "\tif batch_size is None or batch_size >= N:\n",
    "\t\tbatch_size = N\n",
    "\tfor _ in range(epochs):\n",
    "\t\tindices = np.random.permutation(N)\n",
    "\t\tX_s, y_s = X_tr[indices], y_tr[indices]\n",
    "\t\tfor i in range(0, N, batch_size):\n",
    "\t\t\tX_b = X_s[i:i+batch_size]\n",
    "\t\t\ty_b = y_s[i:i+batch_size]\n",
    "\t\t\ty_p = model.forward(X_b)\n",
    "\t\t\tgrads = model.get_gradients(X_b, y_b, y_p)\n",
    "\t\t\toptimizer.update(model.params, grads)\n",
    "\t\ttrain_pred = model.forward(X_tr)\n",
    "\t\ttest_pred = model.forward(X_te)\n",
    "\t\thistory['loss'].append(np.mean((y_tr - train_pred)**2))\n",
    "\t\thistory['acc'].append(np.mean(np.sign(train_pred) == y_tr))\n",
    "\t\thistory['val_loss'].append(np.mean((y_te - test_pred)**2))\n",
    "\t\thistory['val_acc'].append(np.mean(np.sign(test_pred) == y_te))\n",
    "\treturn history\n",
    "\n",
    "def train_gd(model, X, y, X_val, y_val, lr=0.01, epochs=100):\n",
    "\thistory = {'loss': [], 'acc': [], 'val_loss': [], 'val_acc': [], 'time': [], 'weights': []}\n",
    "\tm0 = time.time()\n",
    "\tfor _ in range(epochs):\n",
    "\t\ty_pred = model.forward(X)\n",
    "\t\tgrad = model.get_gradients(X, y, y_pred)\n",
    "\t\tmodel.w -= lr * grad\n",
    "\t\thistory['loss'].append(model.loss(y, y_pred))\n",
    "\t\thistory['acc'].append(model.accuracy(y, y_pred))\n",
    "\t\tval_pred = model.forward(X_val)\n",
    "\t\thistory['val_loss'].append(model.loss(y_val, val_pred))\n",
    "\t\thistory['val_acc'].append(model.accuracy(y_val, val_pred))\n",
    "\t\thistory['time'].append(time.time() - m0)\n",
    "\t\thistory['weights'].append(model.w.flatten().copy())\n",
    "\treturn history\n",
    "\n",
    "def train_sgd(model, X, y, X_val, y_val, lr=0.01, epochs=100):\n",
    "\thistory = {'loss': [], 'acc': [], 'val_loss': [], 'val_acc': [], 'time': [], 'weights': []}\n",
    "\tm0 = time.time()\n",
    "\tN = X.shape[0]\n",
    "\tfor _ in range(epochs):\n",
    "\t\tidx = np.random.permutation(N)\n",
    "\t\tfor i in idx:\n",
    "\t\t\txi = X[i:i+1]\n",
    "\t\t\tyi = y[i:i+1]\n",
    "\t\t\ty_pred = model.forward(xi)\n",
    "\t\t\tgrad = model.get_gradients(xi, yi, y_pred)\n",
    "\t\t\tmodel.w -= lr * grad\n",
    "\t\ttrain_pred = model.forward(X)\n",
    "\t\tval_pred = model.forward(X_val)\n",
    "\t\thistory['loss'].append(model.loss(y, train_pred))\n",
    "\t\thistory['acc'].append(model.accuracy(y, train_pred))\n",
    "\t\thistory['val_loss'].append(model.loss(y_val, val_pred))\n",
    "\t\thistory['val_acc'].append(model.accuracy(y_val, val_pred))\n",
    "\t\thistory['time'].append(time.time() - m0)\n",
    "\t\thistory['weights'].append(model.w.flatten().copy())\n",
    "\treturn history\n",
    "\n",
    "def train_adagrad(model, X, y, X_val, y_val, lr=0.01, epochs=100, eps=1e-8):\n",
    "    history = {'loss': [], 'acc': [], 'val_loss': [], 'val_acc': [], 'time': [], 'weights': []}\n",
    "    m0 = time.time()\n",
    "    G = np.zeros_like(model.w)\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        y_pred = model.forward(X)\n",
    "        grad = model.get_gradients(X, y, y_pred)\n",
    "        \n",
    "        G += grad**2\n",
    "        model.w -= lr * grad / (np.sqrt(G) + eps)\n",
    "\n",
    "        train_pred = model.forward(X)\n",
    "        val_pred = model.forward(X_val)\n",
    "\n",
    "        history['loss'].append(model.loss(y, train_pred))\n",
    "        history['acc'].append(model.accuracy(y, train_pred))\n",
    "        history['val_loss'].append(model.loss(y_val, val_pred))\n",
    "        history['val_acc'].append(model.accuracy(y_val, val_pred))\n",
    "        history['time'].append(time.time() - m0)\n",
    "        history['weights'].append(model.w.flatten().copy())\n",
    "    return history\n",
    "\n",
    "def train_rmsprop(model, X, y, X_val, y_val, lr=0.001, epochs=100, decay_rate=0.9, eps=1e-8):\n",
    "    history = {'loss': [], 'acc': [], 'val_loss': [], 'val_acc': [], 'time': [], 'weights': []}\n",
    "    m0 = time.time()\n",
    "    E_g2 = np.zeros_like(model.w)\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        y_pred = model.forward(X)\n",
    "        grad = model.get_gradients(X, y, y_pred)\n",
    "        \n",
    "        E_g2 = decay_rate * E_g2 + (1 - decay_rate) * (grad**2)\n",
    "        model.w -= lr * grad / (np.sqrt(E_g2) + eps)\n",
    "\n",
    "        train_pred = model.forward(X)\n",
    "        val_pred = model.forward(X_val)\n",
    "\n",
    "        history['loss'].append(model.loss(y, train_pred))\n",
    "        history['acc'].append(model.accuracy(y, train_pred))\n",
    "        history['val_loss'].append(model.loss(y_val, val_pred))\n",
    "        history['val_acc'].append(model.accuracy(y_val, val_pred))\n",
    "        history['time'].append(time.time() - m0)\n",
    "        history['weights'].append(model.w.flatten().copy())\n",
    "    return history\n",
    "\n",
    "def train_adam(model, X, y, X_val, y_val, lr=0.001, epochs=100, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "\thistory = {'loss': [], 'acc': [], 'val_loss': [], 'val_acc': [], 'time': [], 'weights': []}\n",
    "\tm0 = time.time()\n",
    "\tm = np.zeros_like(model.w)\n",
    "\tv = np.zeros_like(model.w)\n",
    "\tt = 0\n",
    "\tfor _ in range(epochs):\n",
    "\t\tt += 1\n",
    "\t\ty_pred = model.forward(X)\n",
    "\t\tgrad = model.get_gradients(X, y, y_pred)\n",
    "\t\tm = beta1*m + (1-beta1)*grad\n",
    "\t\tv = beta2*v + (1-beta2)*(grad**2)\n",
    "\t\tm_hat = m / (1-beta1**t)\n",
    "\t\tv_hat = v / (1-beta2**t)\n",
    "\t\tmodel.w -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "\t\ttrain_pred = model.forward(X)\n",
    "\t\tval_pred = model.forward(X_val)\n",
    "\t\thistory['loss'].append(model.loss(y, train_pred))\n",
    "\t\thistory['acc'].append(model.accuracy(y, train_pred))\n",
    "\t\thistory['val_loss'].append(model.loss(y_val, val_pred))\n",
    "\t\thistory['val_acc'].append(model.accuracy(y_val, val_pred))\n",
    "\t\thistory['time'].append(time.time() - m0)\n",
    "\t\thistory['weights'].append(model.w.flatten().copy())\n",
    "\treturn history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1608468",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_embedding_columns(df):\n",
    "\tq_col = None\n",
    "\tgood_col = None\n",
    "\tbad_col = None\n",
    "\tfor c in df.columns:\n",
    "\t\tlow = c.lower()\n",
    "\t\tif 'soru' in low and 'emb' in low:\n",
    "\t\t\tq_col = c\n",
    "\t\tif ('iyi' in low or 'good' in low) and 'emb' in low:\n",
    "\t\t\tgood_col = c\n",
    "\t\tif ('kotu' in low or 'bad' in low) and 'emb' in low:\n",
    "\t\t\tbad_col = c\n",
    "\t\t\t\n",
    "\tif q_col is None and 'Soru_Embedding' in df.columns:\n",
    "\t\tq_col = 'Soru_Embedding'\n",
    "\tif good_col is None and 'Iyi_Cevap_Embedding' in df.columns:\n",
    "\t\tgood_col = 'Iyi_Cevap_Embedding'\n",
    "\tif bad_col is None and 'Kotu_Cevap_Embedding' in df.columns:\n",
    "\t\tbad_col = 'Kotu_Cevap_Embedding'\n",
    "\treturn q_col, good_col, bad_col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "678348e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_pipeline_for_pickle(pkl_path, epochs_main=EPOCHS_MAIN):\n",
    "\ttag = os.path.splitext(os.path.basename(pkl_path))[0]\n",
    "\tdf = pd.read_pickle(pkl_path)\n",
    "\tq_col, good_col, bad_col = find_embedding_columns(df)\n",
    "\n",
    "\tX_list = []\n",
    "\ty_list = []\n",
    "\tfor _, row in df.iterrows():\n",
    "\t\tq_emb = np.array(row[q_col])\n",
    "\t\tgood_emb = np.array(row[good_col])\n",
    "\t\tpos_input = np.concatenate([q_emb, good_emb])\n",
    "\t\tX_list.append(pos_input)\n",
    "\t\ty_list.append(1.0)\n",
    "\t\tbad_emb = np.array(row[bad_col])\n",
    "\t\tneg_input = np.concatenate([q_emb, bad_emb])\n",
    "\t\tX_list.append(neg_input)\n",
    "\t\ty_list.append(-1.0)\n",
    "\n",
    "\tX = np.array(X_list)\n",
    "\ty = np.array(y_list).reshape(-1, 1)\n",
    "\tones = np.ones((X.shape[0], 1))\n",
    "\tX = np.concatenate([X, ones], axis=1)\n",
    "\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\tinput_dim = X_train.shape[1]\n",
    "\n",
    "\tresults = {'GD': [], 'SGD': [], 'Adam': [], 'AdaGrad': [], 'RMSProp': []}\n",
    "\tweight_trajectories = []\n",
    "\n",
    "\tfor seed_idx in range(NUM_SEEDS):\n",
    "\t\tinitial_w = np.random.randn(input_dim, 1) * 0.05\n",
    "\n",
    "\t\tmodel_gd = actualModel(input_dim, initial_w)\n",
    "\t\thist_gd = train_gd(model_gd, X_train, y_train, X_test, y_test, lr=LEARNING_RATE, epochs=epochs_main)\n",
    "\t\tresults['GD'].append(hist_gd)\n",
    "\t\tweight_trajectories.append((\"GD\", seed_idx, hist_gd['weights']))\n",
    "\n",
    "\t\tmodel_sgd = actualModel(input_dim, initial_w)\n",
    "\t\thist_sgd = train_sgd(model_sgd, X_train, y_train, X_test, y_test, lr=LEARNING_RATE*0.1, epochs=epochs_main)\n",
    "\t\tresults['SGD'].append(hist_sgd)\n",
    "\t\tweight_trajectories.append((\"SGD\", seed_idx, hist_sgd['weights']))\n",
    "\n",
    "\t\tmodel_adam = actualModel(input_dim, initial_w)\n",
    "\t\thist_adam = train_adam(model_adam, X_train, y_train, X_test, y_test, lr=LEARNING_RATE, epochs=epochs_main)\n",
    "\t\tresults['Adam'].append(hist_adam)\n",
    "\t\tweight_trajectories.append((\"Adam\", seed_idx, hist_adam['weights']))\n",
    "\n",
    "\t\tmodel_adagrad = actualModel(input_dim, initial_w)\n",
    "\t\thist_adagrad = train_adagrad(model_adagrad, X_train, y_train, X_test, y_test, lr=LEARNING_RATE*0.5, epochs=epochs_main)\n",
    "\t\tresults['AdaGrad'].append(hist_adagrad)\n",
    "\t\tweight_trajectories.append((\"AdaGrad\", seed_idx, hist_adagrad['weights']))\n",
    "\t\t\n",
    "\t\tmodel_rmsprop = actualModel(input_dim, initial_w)\n",
    "\t\thist_rmsprop = train_rmsprop(model_rmsprop, X_train, y_train, X_test, y_test, lr=LEARNING_RATE*0.01, epochs=epochs_main)\n",
    "\t\tresults['RMSProp'].append(hist_rmsprop)\n",
    "\t\tweight_trajectories.append((\"RMSProp\", seed_idx, hist_rmsprop['weights']))\n",
    "\n",
    "\tdef get_avg_history(algo_results, key):\n",
    "\t\treturn np.mean(np.array([r[key] for r in algo_results]), axis=0)\n",
    "\n",
    "\talgos = ['GD', 'SGD', 'Adam', 'AdaGrad', 'RMSProp']\n",
    "\tcolors = {'GD': 'blue', 'SGD': 'orange', 'Adam': 'green', 'AdaGrad': 'red', 'RMSProp': 'purple'}\n",
    "\n",
    "\tfig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\tax = axes[0,0]\n",
    "\tfor algo in algos:\n",
    "\t\tax.plot(get_avg_history(results[algo], 'loss'), label=algo, color=colors[algo])\n",
    "\tax.set_title(\"Average Training Loss\"); ax.grid(); ax.legend()\n",
    "\n",
    "\tax = axes[0,1]\n",
    "\tfor algo in algos:\n",
    "\t\tax.plot(get_avg_history(results[algo], 'acc'), label=algo, color=colors[algo])\n",
    "\tax.set_title(\"Average Training Accuracy\"); ax.grid(); ax.legend()\n",
    "\n",
    "\tax = axes[1,0]\n",
    "\tfor algo in algos:\n",
    "\t\tax.plot(get_avg_history(results[algo], 'val_loss'), label=algo, color=colors[algo])\n",
    "\tax.set_title(\"Average Test Loss\"); ax.grid(); ax.legend()\n",
    "\n",
    "\tax = axes[1,1]\n",
    "\tfor algo in algos:\n",
    "\t\tax.plot(get_avg_history(results[algo], 'val_acc'), label=algo, color=colors[algo])\n",
    "\tax.set_title(\"Average Test Accuracy\"); ax.grid(); ax.legend()\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\tout_path = os.path.join(IMGS_DIR, f\"{tag}_optimization_metrics_combined.png\")\n",
    "\tplt.savefig(out_path)\n",
    "\tplt.close(fig)\n",
    "\n",
    "\tflat_weights = []\n",
    "\tfor algo, seed, w_list in weight_trajectories:\n",
    "\t\tfor w in w_list:\n",
    "\t\t\tflat_weights.append(w)\n",
    "\tflat_weights = np.array(flat_weights)\n",
    "\tperplexity = min(30, flat_weights.shape[0] // 10)\n",
    "\tperplexity = max(5, perplexity)\n",
    "\ttsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "\tW2D = tsne.fit_transform(flat_weights)\n",
    "\n",
    "\tplt.figure(figsize=(12,10))\n",
    "\tpoints_per_run = len(weight_trajectories[0][2])\n",
    "\tfor idx, (algo, seed, _) in enumerate(weight_trajectories):\n",
    "\t\tstart = idx * points_per_run\n",
    "\t\tend = start + points_per_run\n",
    "\t\tpts = W2D[start:end]\n",
    "\t\tplt.plot(pts[:,0], pts[:,1], color=colors[algo], alpha=0.5)\n",
    "\t\tplt.scatter(pts[0,0], pts[0,1], color=colors[algo], marker='x', s=50)\n",
    "\t\tplt.scatter(pts[-1,0], pts[-1,1], color=colors[algo], marker='o', s=50)\n",
    "\tplt.title(\"t-SNE Optimization Trajectories\")\n",
    "\tplt.grid(); plt.legend(algos)\n",
    "\tout_path = os.path.join(IMGS_DIR, f\"{tag}_optimization_tsne_trajectories.png\")\n",
    "\tplt.savefig(out_path)\n",
    "\tplt.close()\n",
    "\n",
    "\tsize_results = []\n",
    "\tfor s in SIZES:\n",
    "\t\tsubset = int(len(X_train) * s)\n",
    "\t\tif subset < 10: continue\n",
    "\t\tidx = np.random.permutation(len(X_train))\n",
    "\t\tX_sub = X_train[idx[:subset]]\n",
    "\t\ty_sub = y_train[idx[:subset]]\n",
    "\t\tmlp = TwoLayerMLP(input_dim, hidden_dim=32)\n",
    "\t\topt = AdamOpt(lr=0.001)\n",
    "\t\thist = train_generic(mlp, opt, X_sub, y_sub, X_test, y_test, epochs=EPOCHS_MLP_SMALL)\n",
    "\t\tsize_results.append(hist['val_acc'][-1])\n",
    "\tplt.figure(figsize=(8,5))\n",
    "\tplt.plot([s*100 for s in SIZES[:len(size_results)]], size_results, marker='o')\n",
    "\tplt.title(\"Dataset Size vs Test Accuracy\")\n",
    "\tplt.xlabel(\"Dataset Percentage\")\n",
    "\tplt.ylabel(\"Accuracy\")\n",
    "\tplt.grid()\n",
    "\tout_path = os.path.join(IMGS_DIR, f\"{tag}_exp_dataset_size.png\")\n",
    "\tplt.savefig(out_path)\n",
    "\tplt.close()\n",
    "\n",
    "\tmlp_big = TwoLayerMLP(input_dim, hidden_dim=64)\n",
    "\tperc = actualModel(input_dim)\n",
    "\thist_mlp = train_generic(mlp_big, AdamOpt(lr=0.001), X_train, y_train, X_test, y_test, epochs=EPOCHS_MLP_COMPARE)\n",
    "\thist_perc = train_adam(perc, X_train, y_train, X_test, y_test, lr=0.001, epochs=EPOCHS_MLP_COMPARE)\n",
    "\tplt.figure(figsize=(8,5))\n",
    "\tplt.plot(hist_mlp['val_acc'], label=\"MLP\", color=\"red\")\n",
    "\tplt.plot(hist_perc['val_acc'], label=\"Normal Model\", color=\"blue\")\n",
    "\tplt.title(\"MLP vs Normal Model Test Accuracy\")\n",
    "\tplt.grid()\n",
    "\tplt.legend()\n",
    "\tout_path = os.path.join(IMGS_DIR, f\"{tag}_exp_mlp_vs_normal.png\")\n",
    "\tplt.savefig(out_path)\n",
    "\tplt.close()\n",
    "\n",
    "\tresults_mlp = {'GD': [], 'SGD': [], 'Adam': [], 'AdaGrad': [], 'RMSProp': []}\n",
    "\tnum_seeds_comp = 3\n",
    "\tfor run in range(num_seeds_comp):\n",
    "\t\tm_gd = TwoLayerMLP(input_dim, hidden_dim=64)\n",
    "\t\th_gd = train_generic(m_gd, SGDOpt(lr=LEARNING_RATE), X_train, y_train, X_test, y_test, epochs=epochs_main, batch_size=None)\n",
    "\t\tresults_mlp['GD'].append(h_gd)\n",
    "\t\tm_sgd = TwoLayerMLP(input_dim, hidden_dim=64)\n",
    "\t\th_sgd = train_generic(m_sgd, SGDOpt(lr=LEARNING_RATE*0.1), X_train, y_train, X_test, y_test, epochs=epochs_main, batch_size=1)\n",
    "\t\tresults_mlp['SGD'].append(h_sgd)\n",
    "\t\tm_adam = TwoLayerMLP(input_dim, hidden_dim=64)\n",
    "\t\th_adam = train_generic(m_adam, AdamOpt(lr=0.001), X_train, y_train, X_test, y_test, epochs=epochs_main, batch_size=32)\n",
    "\t\tresults_mlp['Adam'].append(h_adam)\n",
    "\t\tm_adagrad = TwoLayerMLP(input_dim, hidden_dim=64)\n",
    "\t\th_adagrad = train_generic(m_adagrad, AdaGradOpt(lr=LEARNING_RATE*0.5), X_train, y_train, X_test, y_test, epochs=epochs_main, batch_size=32)\n",
    "\t\tresults_mlp['AdaGrad'].append(h_adagrad)\n",
    "\t\tm_rmsprop = TwoLayerMLP(input_dim, hidden_dim=64)\n",
    "\t\th_rmsprop = train_generic(m_rmsprop, RMSPropOpt(lr=LEARNING_RATE*0.01), X_train, y_train, X_test, y_test, epochs=epochs_main, batch_size=32)\n",
    "\t\tresults_mlp['RMSProp'].append(h_rmsprop)\n",
    "\n",
    "\n",
    "\tfig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\tmetrics = [(\"loss\", \"Avg Training Loss\"), (\"acc\", \"Avg Training Accuracy\"),\n",
    "\t\t\t\t(\"val_loss\", \"Avg Test Loss\"), (\"val_acc\", \"Avg Test Accuracy\")]\n",
    "\tfor i, (metric, title) in enumerate(metrics):\n",
    "\t\tax = axes[i//2][i%2]\n",
    "\t\tfor algo in algos:\n",
    "\t\t\tax.plot(np.mean(np.array([r[metric] for r in results_mlp[algo]]), axis=0),\n",
    "\t\t\t\t\tlabel=f\"MLP-{algo}\", color=colors[algo])\n",
    "\t\tax.set_title(title); ax.grid(); ax.legend()\n",
    "\tplt.tight_layout()\n",
    "\tout_path = os.path.join(IMGS_DIR, f\"{tag}_mlp_metrics_combined.png\")\n",
    "\tplt.savefig(out_path)\n",
    "\tplt.close()\n",
    "\n",
    "\treturn {\n",
    "\t\t\"tag\": tag,\n",
    "\t\t\"results_normal\": results,\n",
    "\t\t\"results_mlp\": results_mlp,\n",
    "\t\t\"avg_val_acc_normal\": {a: np.mean(np.array([r['val_acc'] for r in results[a]]), axis=0) for a in algos}\n",
    "\t}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf460321",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_runs = []\n",
    "if not os.path.exists(IMGS_DIR): os.makedirs(IMGS_DIR)\n",
    "for p in EMBEDDING_PICKLES:\n",
    "\trun_summary = run_pipeline_for_pickle(p, epochs_main=EPOCHS_MAIN)\n",
    "\tall_runs.append(run_summary)\n",
    "if len(all_runs) >= 2:\n",
    "\talgos = ['GD', 'SGD', 'Adam', 'AdaGrad', 'RMSProp']\n",
    "\tplt.figure(figsize=(15, 4))\n",
    "\tfor i, algo in enumerate(algos):\n",
    "\t\tplt.subplot(1, len(algos), i+1)\n",
    "\t\tfor run in all_runs:\n",
    "\t\t\tavg = run['avg_val_acc_normal'][algo]\n",
    "\t\t\tlabel = run['tag']\n",
    "\t\t\tplt.plot(avg, label=label)\n",
    "\t\tplt.title(f\"{algo}: Normal Test Accuracy by Embedding Model\")\n",
    "\t\tplt.xlabel(\"Epoch\")\n",
    "\t\tplt.ylabel(\"Accuracy\")\n",
    "\t\tplt.grid()\n",
    "\t\tplt.legend()\n",
    "\tplt.tight_layout()\n",
    "\tplt.savefig(os.path.join(IMGS_DIR, \"crossmodel_normal_valacc_comparison.png\"))\n",
    "\tplt.close()\n",
    "\n",
    "\tplt.figure(figsize=(15, 5))\n",
    "\twidth = 0.15\n",
    "\tx = np.arange(len(algos))\n",
    "\tfor i, run in enumerate(all_runs):\n",
    "\t\tfinals = [ run['avg_val_acc_normal'][a][-1] for a in algos ]\n",
    "\t\tplt.bar(x + i*width, finals, width=width, label=run['tag'])\n",
    "\tplt.xticks(x + width*(len(all_runs)-1)/2, algos)\n",
    "\tplt.ylabel(\"Final Test Accuracy\")\n",
    "\tplt.title(\"Final Normal Test Accuracy by Optimizer and Embedding Model\")\n",
    "\tplt.legend()\n",
    "\tplt.grid(axis='y')\n",
    "\tplt.tight_layout()\n",
    "\tplt.savefig(os.path.join(IMGS_DIR, \"crossmodel_normal_final_acc_bars.png\"))\n",
    "\tplt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
